{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import statistics\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_to_label={0:'csai',1:'cse',2:'ece'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('ai_assignment5_data.csv')\n",
    "df=df.drop(df.columns[0],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(s):\n",
    "    tokenized_sent=word_tokenize(s)\n",
    "    removing_stop=[]\n",
    "    for j in tokenized_sent:\n",
    "        if(j not in stop_words):\n",
    "            removing_stop.append(lemmatizer.lemmatize(j))\n",
    "    \n",
    "    final_string=' '.join(removing_stop)\n",
    "    return(final_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Bag of Words Implementation with Naive Bayes and MLP Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=[]\n",
    "\n",
    "for i in range(len(df)):\n",
    "    final_string=remove_stop_words(df['text'][i].lower())\n",
    "    corpus.append(final_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer()\n",
    "count_corpus = count_vectorizer.fit_transform(corpus)\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_corpus = tfidf_vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['advanced',\n",
       " 'ai',\n",
       " 'algorithm',\n",
       " 'artificial',\n",
       " 'career',\n",
       " 'computer',\n",
       " 'cv',\n",
       " 'data',\n",
       " 'database',\n",
       " 'dbms',\n",
       " 'deep',\n",
       " 'dsa',\n",
       " 'eld',\n",
       " 'fascinated',\n",
       " 'intelligence',\n",
       " 'interest',\n",
       " 'language',\n",
       " 'learning',\n",
       " 'lie',\n",
       " 'like',\n",
       " 'machine',\n",
       " 'management',\n",
       " 'ml',\n",
       " 'natural',\n",
       " 'nlp',\n",
       " 'processing',\n",
       " 'programming',\n",
       " 'pursue',\n",
       " 'really',\n",
       " 'signal',\n",
       " 'sn',\n",
       " 'structure',\n",
       " 'system',\n",
       " 'vision',\n",
       " 'vlsi',\n",
       " 'want',\n",
       " 'wireless']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 37)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_corpus.toarray().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 37)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_corpus.toarray().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb1=MultinomialNB()\n",
    "mnb1.fit(count_corpus, df['label'])\n",
    "\n",
    "mnb2=MultinomialNB()\n",
    "mnb2.fit(tfidf_corpus, df['label'])\n",
    "\n",
    "mlp1 = MLPClassifier(max_iter=300).fit(count_corpus, df['label'])\n",
    "mlp2 = MLPClassifier(max_iter=300).fit(tfidf_corpus, df['label'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using POS Tagging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cgpa(s):\n",
    "    wordsList = nltk.word_tokenize(s)\n",
    "    pos_tags = nltk.pos_tag(wordsList)\n",
    "    \n",
    "    for i in pos_tags:\n",
    "        if(i[1]=='CD'):   #if the pos tag is cardinal digit it is cgpa\n",
    "            return(i[0])\n",
    "    \n",
    "    return('7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tell about your academic subjects and your cgpa: i like studying data structures and algorithms and my grade point is 9.9\n"
     ]
    }
   ],
   "source": [
    "inp=input(\"Tell about your academic subjects and your cgpa: \")\n",
    "\n",
    "inp=inp.lower()\n",
    "cgpa=get_cgpa(inp)\n",
    "inp=remove_stop_words(inp)\n",
    "inp=[inp]\n",
    "\n",
    "inp1=count_vectorizer.transform(inp).toarray()\n",
    "inp2=tfidf_vectorizer.transform(inp).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction using count vectorizer (Naive Bayes): cse\n",
      "Prediction using tfidf vectorizer (Naive Bayes): cse\n"
     ]
    }
   ],
   "source": [
    "pred1=mnb1.predict(inp1)[0]\n",
    "pred2=mnb2.predict(inp2)[0]\n",
    "\n",
    "print(\"Prediction using count vectorizer (Naive Bayes): \"+str(pred_to_label[pred1]))\n",
    "print(\"Prediction using tfidf vectorizer (Naive Bayes): \"+str(pred_to_label[pred2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction using count vectorizer (ANN): cse\n",
      "Prediction using tfidf vectorizer (ANN): cse\n"
     ]
    }
   ],
   "source": [
    "pred3=mlp1.predict(inp1)[0]\n",
    "pred4=mlp1.predict(inp2)[0]\n",
    "\n",
    "print(\"Prediction using count vectorizer (ANN): \"+str(pred_to_label[pred3]))\n",
    "print(\"Prediction using tfidf vectorizer (ANN): \"+str(pred_to_label[pred4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final prediction: cse\n"
     ]
    }
   ],
   "source": [
    "#finally taking a max vote\n",
    "\n",
    "final_ans=[]\n",
    "final_ans.append(pred1)\n",
    "final_ans.append(pred2)\n",
    "final_ans.append(pred3)\n",
    "final_ans.append(pred4)\n",
    "\n",
    "maxi = statistics.mode(final_ans)\n",
    "\n",
    "print(\"Final prediction: \"+pred_to_label[maxi])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Bhavya'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(\"C:/Users/Bhavya/ai_assigment5.txt\"):\n",
    "    os.remove(\"C:/Users/Bhavya/ai_assigment5.txt\")\n",
    "    print(\"Deleted\")\n",
    "else:\n",
    "    print(\"The file does not exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = open(\"C:/Users/Bhavya/ai_assigment5.txt\",\"w+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_write='interest('+pred_to_label[maxi]+','+cgpa+').'\n",
    "file1.write(to_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "interest(cse,9.9).\n"
     ]
    }
   ],
   "source": [
    "print(to_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
